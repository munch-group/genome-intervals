
# Laptop setup

You will work on our computing cluster, but before we get there, we must get you properly set up on your machine. 

### The Terminal

The programs you will use in the project are command-line applications. I.e., programs executed by writing their name and any arguments in a "terminal" rather than clicking on an icon and using a graphical user interface. Many different programs can serve as a terminal.

- If you have a Windows machine, use the *Windows Powershell* app.
- If you have a Mac, the terminal you will use is called *Terminal*.

From now on, whenever I refer to the terminal, I mean *Anaconda Powershell Prompt* on Windows and *Terminal* on Mac. I will assume some familiarity with using a terminal and executing commands on the command line. If you have not used a terminal before, or if you are a bit rusty, you should run through [this primer](https://lifehacker.com/5633909/who-needs-a-mouse-learn-to-use-the-command-line-for-almost-anything) before you go on.

### Pixi

First install [Pixi](https://pixi.sh/latest/installation/) if you did not do so already.


# Install gh, git and maybe slurm-jupyter




# GitHub account and munch-group membership

<img src="https://github.githubassets.com/images/modules/open_graph/github-mark.png" align="right" width="200"  />

[Git](https://git-scm.com/) is a version control tool that you use from the terminal. A folder under Git control is called a repository. Git does not interfere with your files and it does not save them. It lets you monitor the state of your files so you can easily see if any files are added, modified, or removed, and it allows you to (manually) maintain a record of what files where changes when, how, and for what reason. 

1. Start by creating your own [github account](https://github.com/join) if you do not have one already.
2. Email me your GitHub username on so I can add you to the shared GitHub account for our research group. I will create a repository for you with scaffold of folders and with placeholder files that will get you started in the right way.
3. Wait for an email or notification and accept my inviation to the "munch-lab" GitHub organization and an email from me with the name of the repository I made for your project. On these pages, I will use `<repositoryname>` for the name of your repository.


# Cluster access

::: callout-warning
## This is not a leisure read

Make sure you follow it to the letter. Do each step in order, and do not proceed until you have successfully completed each step.
:::

<img src="https://icon-library.com/images/62418-database-icons-virtual-servers-computer-private-server.png" align="right" width="150"/>

The [GenomeDK](https://genome.au.dk/) cluster is a huge collection of computers with a shared file system. You can find lots of helpful information about the cluster on their homepage beyond what I cover on these pages. The cluster does not have a screen or keyboard you can use, but by connecting to the cluster from your computer, you can create and edit files much like if they were on your laptop. These pages take you through the steps to connect and access the cluster.

Below, you will see something like `<cluster user name>` or `<project folder>`. Whenever you see something like that, you should replace everything, including `<` and `>`, with whatever it says. E.g., if your cluster user name is `mike`, you should replace `<cluster user name>` with `mike`.

Before you can begin, you need access to the cluster. The cluster is called GenomeDK and has its own [website](https://genome.au.dk) with lots of information and documentation. To get an account on the cluster, you must [request one here](https://genome.au.dk/docs/getting-started/#request-access). Email me your cluster user name once you get it so I can add you as member of your project folder. 

elow, `<username>` will represent your user name.

### Connecting to the cluster using ssh

<img src="https://atulhost.com/wp-content/uploads/2020/04/ssh.png" align="right" width="300" height="200"/>

SSH is short for secure shell. A shell is the software that lets you run commands in your terminal window. The *secure shell* (SSH) allows you to log in to another computer to navigate the folders and run commands on that machine. So when you open your terminal window, your commands run on your local machine, but when you "ssh" (yes, it is a verb, too) into the cluster, your commands run on the cluster. Before you go on, try to run the command `hostname` in your terminal. You can see that it prints something that tells you that you are on your laptop.

You connect to the cluster from the terminal by executing the command below (remember to replace `<cluster user name>` with your actual cluster user name):

``` {.bash filename="Terminal"}
ssh <cluster user name>@login.genome.au.dk
```

When you do, ssh prompts you for the password that goes with your cluster username (GenomeDK requires two-factor authentication and will sometimes ask you for a site key). Enter the password and press Enter. You are now in your home folder on the cluster. Your terminal looks the same as before, but it will print:

```         
  _____                                ______ _   __
 |  __ \                               |  _  \ | / /
 | |  \/ ___ _ __   ___  _ __ ___   ___| | | | |/ /
 | | __ / _ \ '_ \ / _ \| '_ ` _ \ / _ \ | | |    \
 | |_\ \  __/ | | | (_) | | | | | |  __/ |/ /| |\  \
  \____/\___|_| |_|\___/|_| |_| |_|\___|___/ \_| \_/
```

If you run the `hostname` command again, you can see that you are now on `fe-open-01`. Now log out of the cluster again by typing `exit` and Enter (or pressing Ctrl-d). You are now back on your laptop. Try `hostname` again and see the name of your computer.

You will need to log in to the cluster many times, so you should set up your SSH connection to the cluster so you can connect securely without typing the password every time. This is roughly how it works:

::: callout-note
## SSH keys

Firstly, you have to understand what public/private encryption keys are. A *private* key is a very long, random sequence of bits. A private key is kept secret and never leaves your laptop. A *public* key is another string of bits that is a derivative of the private key.

You can generate a unique public key from the private key but cannot get the private key from a public key: It is a one-way process. You can encrypt (or sign) any message using the public key, and it will only be possible to decrypt it using the private key it is derived from. In other words, anyone with your public key can send you encrypted messages that only you will be able to read.

So, if the cluster has your public key saved, it can authenticate you like this: The cluster sends your laptop a message encrypted using the public key. Your laptop then decrypts the message using its private key and sends it back. If the cluster receives a correctly decrypted message it knows it is you and logs you in.
:::

First, check if you have these two authentication files on your local machine:

```         
~/.ssh/id_rsa
~/.ssh/id_rsa.pub
```

You can do so using the `ls` commmand:

``` {.bash filename="Terminal"}
ls -a ~/.ssh
```

You most likely do not. If so, you generate authentication keys with the command below. Just press Enter when prompted for a file in which to save the key. Do not enter a passphrase when prompted - just press enter:

``` {.bash filename="Terminal"}
ssh-keygen -t rsa
```

Now use `ssh` to create a directory `~/.ssh` on the cluster (assuming your username on the cluster is `<cluster user name>`). SSH will prompt you for your password.

``` {.bash filename="Terminal"}
ssh <cluster user name>@login.genome.au.dk mkdir -p .ssh
```

Finally, append the public `ssh` key on your local machine to the file `.ssh/authorized_keys` on the cluster and enter your password (replace `<cluster user name>` with your cluster user name):

``` {.bash filename="Terminal"}
cat ~/.ssh/id_rsa.pub | ssh username@login.genome.au.dk 'cat >> .ssh/authorized_keys'
```

From now on, you can log into the cluster from your local machine without being prompted for a password.

Try it:

``` {.bash filename="Terminal"}
ssh <cluster user name>@login.genome.au.dk
```

(see, no password).

# Pixi 


## The Pixi environment



# Cluster project folder

The project folder is a folder that is set up on the cluster to hold your project. I use `<projectfolder>` as placeholder for the projectname.

It is accessible to only you and anyone else you collaborate with (such as your supervisor). The project folder is in your home directory and has the following subfolders:

```txt
<projectfolder>
    /data
    /people
        /<username>
        /<supervisor_username>
```

The name of the project folder is is also the name of the account that gets billed for the work on the cluster. When you run `gwf`, `srun`, `sbatch` or `slurm-jupyter` (see below) you must specify that project name using the `-A` or `--account` options (see below for more details on that). 

# Project GitHub repository

Start logging into the cluster and run these two commands to let Git know who you are:

```{.bash filename="Terminal"}
git config --global user.name "<Your GitHub user name>"
git config --global user.email <your_email@whatever.com>
```

To be able to access GitHub vis SSH, you must add an ssh key to your github account. If you have not done so already, you can do it by logging into the cluster and run this command:

```
curl -fsSL https://gist.githubusercontent.com/kaspermunch/e8f5a3b5ba93bec991b4a256797bd039/raw/1704d96e0800fb86bbd46015a444a95f553bab70/gitHub-ssh-key.sh > tmp.sh && bash tmp.sh
```

Now you can navigate to your own folder inside the project folder (`<projectfolder>/people/<username>`). In there, you will your project repository that I already put there (cloned) for you. 

**This is important:** Everything you do and all the files you save should be in the github repository folder. 


You now have a folder called `<projectfolder>/people/<username>/<repositoryname>` and this is where you must keep *all* your files for the project.

If you `cd` into `<repositoryname>` and run `ls`, you will see a number of folders.

- `data`: Stores *small* (tens of megabases) data files you want to keep .
- `sandbox`: Stores experiment and other files that are not yet part of your project workflow. This keeps the rest of the folder structure clean.
- `scripts`: Stores Python scripts that that produces intermediate and final results.
- `steps`: Stores intermediary files ("steps" on the way to final results).
- `notebooks`: Stores Juptyer notebooks with code, documentation, and results.
- `figures`: Stores result plots and figures you make.
- `results`: Stores the small result files of your project (tens of megabases).
- `reports`: Stores documents reporting your findings.

Files in all those folders are under Git control, *except* files in the `steps` folder. Those files are not backed up in any way, but should instead be reproducible using the code and information in your other folders.

::: {.callout-important}
## Warning: Files on the cluster are not backed up!

Your files on the cluster are not backed up! If you want to backup files, you need to put them in a folder called BACKUP. But even if you do you may loose a week of work, since the backup loop is very slow. 
:::

The best way to keep your progress safe, is to ensure is reproducible and pushed to GitHub as often as it makes sense (at least onece a day). The more often you do it, the less work you will loose if you accidentally delete or overwrite a file. More about that in @sec-reproducible-reserach and @sec-git-101.

# The Pixi environment....



# How to run a Jupyter notebook on the cluster

Jupyter runs best in the [Chrome browser](https://www.google.com/chrome) or Safari on Mac. For the best experience, install that before you go on. It does not need to be your default browser. `slurm-jupyter` will use it anyway. Now make sure you are on your own machine and that your `popgen` environment is activated. Then run this command to start a jupyter notebook on the cluster and send the display to your browser:

```{.bash filename="Terminal"}
slurm-jupyter -u <cluster_user_name> -A <projectfolder<> -e birc-project --chrome
```

(replace `<cluster_user_name>` with your cluster user name, `<projectfolder>` with your project folder name).

Watch the terminal to see what is going on. After a while, a jupyter notebook should show up in your browser window. The first time you do this, your browser may refuse to show jupyter because the connection is unsafe. In Safari you are prompted with this winidow where you click "details":

<img src="img/mac_warning1.png" alt="image" width="450"/>

Then you get this window and click "visit this website":

<img src="img/mac_warning2.png" alt="image" width="450"/>

In Chrome, you can simply type the characters "thisisunsafe" while in the Chrome window:

<img src="img/thisisunsafe.png" alt="image" width="450"/>

Once ready, jupyter may ask for your cluster password. To close the jupyter notebook, press `Ctrl-c` in the terminal. Closing the browser window does **not** close down the jupyter on the cluster. You can [read this tutorial](https://www.dataquest.io/blog/jupyter-notebook-tutorial/) to learn how to use a jupyter notebook.

# Visual Studio Code

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Visual_Studio_Code_1.35_icon.svg/2560px-Visual_Studio_Code_1.35_icon.svg.png" align="right" width="100" height="100" />

You should install Visual Studio Code. VScode is great for developing scripts and editing text files. Once you have installed VScode, you should install the "Remote Development" extension. You do that by clicking the funny squares in the left bar and search for "Remote Development". Once installed, you can click the small green square in the lower-left corner to connect to the cluster. Select "Connect current window to host" then "Add new SSH host", then type `<username>@login.genome.au.dk`, then select the config file `.ssh/config`. Now you can click the small green square in the lower-left corner to connect to the cluster by selecting `login.genome.au.dk`. It may take a bit, but once it is done installing a remote server, you will have access to the files in your home folder on the cluster.

### Running interactive commands on the cluster

When you log into the cluster, you land on the "front-end" of the cluster. Think of it as the lobby of a giant hotel. If you execute the `hostname` command, you will get `fe-open-01`. `fe1` is the name of the front-end machine. The "front-end" is a single machine shared by anyone who logs in. So you cannot run resource-intensive jobs there, but quick commands are ok. Commands that finish in less than ten seconds are ok. In the exercises for this course, you will run software that takes a much longer time to finish. So you need one of the computing machines on the cluster, so you can work on that instead. You ask for a computing machine by running this command:

```{.bash filename="Terminal"}
srun --mem-per-cpu=1g --time=3:00:00 --account=<projectfolder> --pty bash
```

That says that you need at most one gigabyte of memory, that you need it for at most three hours (the duration of the exercise), and that the computing expenses should be billed to the project `<projectfolder>`. When you execute the command, your terminal will say "srun: job 40924828 queued and waiting for resources". That means that you are waiting for a machine. Once it prints "srun: job 40924828 has been allocated resources", you have been logged into a computing node. If you execute the `hostname` command, you will get something like `s05n20`. `s05n20` is a computing machine. The same way you moved from your own computer to front-end machine of the cluster by logging in using ssh, the command above moves you from the front-end to a compute machine. Now you can execute any command you like without causing trouble for anyone.

Now try to log out of the compute node by executing the `exit` command or by pressing `Ctrl-d`. If you execute the `hostname` command again, you will get `fe1.genomedk.net` showing that you are back at the front-end machine.

### Queueing commands on the cluster

For non-interactive work, it is better to submit your command as a job to the cluster. When you do that, the job gets queued along with many other jobs, and as soon as the requested resources are available on the cluster, the job will start on one the many many machines. To submit a job, you must first create a file (a "batch script") that contains both the requested computer resources and the command you want to run. 

Create a file called `myscript.sh` with exactly this content:

```txt
#!/bin/bash
#SBATCH --mem=1gb
#SBATCH --time=01:00:00
#SBATCH --account=<projectfolder>
#SBATCH --job-name=firstjob

echo "I can submit cluster jobs now!" > success.txt
```

(replace `<projectfolder>` with your project folder name)

The first line says this is a bash script, the lines following three lines say that your job needs at most one gigabyte of memory, will run for at most one hour, that the expenses should be billed to the project `<projectfolder>`. The fourth line gives the name of the job. Here we have called it `firstjob`, but you should name it something sensible. 

You submit the job using the `sbatch` command: 

```{.bash filename="Terminal"}
sbatch myscript.sh
```

Now your job is queued. Use the `mj` command to see what jobs you have queued or running. That will show something like this:

```txt
                                                                        Alloc
Job ID           Username Queue    Jobname    SessID NDS  S Elap Time   nodes
---------------- -------- -------- ---------- ------ ---  - ----------  -----
34745986         kmt      normal   firstjob       --   1  R 0-00:19:27  s03n56
```

If you want to cancel this job before it finishes, you can use the `scancel` command:

```{.bash filename="Terminal"}
scancel 34745986
```

Once your job finishes, it has created the file `success.txt` and written "I can submit cluster jobs now!" to it. So see that you can use the `cat` command:

```{.bash filename="Terminal"}
cat success.txt
```

When you a program or script on the command line, it usually also prints some information in the terminal. When you run a job on the cluster there is no terminal to print to. Instead, this is written to two files that you can read when the job finishes. In this case, the fiels are called `firstjob.stdout` and `firstjob.stderr`. To see what is in them, you can use the `cat` command:

```{.bash filename="Terminal"}
cat firstjob.stdout
```

and

```{.bash filename="Terminal"}
cat firstjob.stderr
```

That is basically it. 

### How to copy files to and from the cluster

You may need to transfer files back and forth between your own machine and the cluster. To copy a file called `file` in a directory called `dir` on the cluster to the current folder on your own machine, you can use the `scp` command:

```{.bash filename="Terminal"}
scp <cluster_user_name>@login.genome.au.dk:dir/file .
```

To copy a file called `file` in the current folder on your own machine to a folder called `dir` on the cluster, you do this:

```{.bash filename="Terminal"}
scp ./file <cluster_user_name>@login.genome.au.dk:dir/
```


